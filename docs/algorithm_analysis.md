# 分布式 DNN 推理调度算法结果差异深度分析

在对三种分布式 DNN 推理算法（DINA, MEDIA, Ours）进行仿真测试后，我们发现对于大多数模型，DINA 与 MEDIA 的推理延迟（Latency）结果完全一致，而 Ours 算法在复杂 DAG 模型（如 InceptionV3）上表现更优。本文档对这一现象背后的技术原因进行深入解析。

## 1. 核心现象：DINA 与 MEDIA 的“结果对齐”

在 100 Mbps 带宽环境下，DINA 和 MEDIA 在 InceptionV3, BERT, ViT 等模型上的仿真时延完全相同。这主要源于以下两个技术陷阱：

### A. 拓扑线性化陷阱 (Linearization Trap)
DINA 和 MEDIA 在分区阶段首先使用 `topological_sort` 将 DAG 图转换为一个线性序列。
- **逻辑缺陷**：当算法沿着这个序列进行贪心合并时，它会无视原图中的分叉（Fork）和汇聚（Join）结构。
- **后果**：原本在 Inception 模块中可以并行的独立分支，被强制“压扁”成了一段顺序执行的序列。这样生成的分区在逻辑上是严格串行的。

### B. 分区粒度过粗 (Coarse Granularity)
- **容量对比**：InceptionV3 模型的总内存约 275MB，而 EPC 有效容量为 93MB。
- **合并行为**：
  - **DINA**：只要内存不到 93MB 就继续合并。
  - **MEDIA**：虽然允许超限，但在 100 Mbps 带宽下，跨机传输 1MB 的代价（约 80ms）低于“超限后 50% 性能下降”带来的计算损失，因此 MEDIA 也选择了 93MB 左右的合并点。
- **最终形态**：两算法都将约 180 层的模型压缩成了 **3 个巨大的顺序依赖分区**。由于只有 3 个串行任务，分布式集群退化为单机顺序执行，时延等于所有层耗时的简单总和（1505 ms）。

## 2. 为什么 Ours 展现了显著优势？

在相同环境下，Ours 在 InceptionV3 上跑出了 **1367 ms** 的成绩，显著优于 DINA/MEDIA 的 1505 ms。

### A. 拓扑感知分区 (Topology-Aware Partitioning)
Ours 在合并层时遵循一个硬性限制：**仅合并出度为 1 且入度为 1 的边**。
- **效果**：它识别并严格保留了 Inception 模块中的并发分支结构。
- **结果**：Ours 将模型分成了 **54 个细粒度分区**，而不是 3 个。

### B. 并行调度收益 > 通信开销
- 54 个分区虽然增加了跨机通信的次数，但在 100 Mbps 的主流带宽下，跨机同步的延迟微乎其微。
- **核心收益**：多台服务器能够同时处理 54 个分区中互不相关的分支（模型并行）。这种并行的计算收益抵消并超过了增加的通信开销，成功打破了 1505 ms 的串行瓶颈。

## 3. 算法差异性的触发条件分析

为了在实验中展现 MEDIA 与 DINA 的差异，需要调整环境参数以触发它们的边界逻辑：

| 变量 | 效果 | 预期表现 |
| :--- | :--- | :--- |
| **带宽降至 1-10 Mbps** | 通信开销变得极度昂贵 | MEDIA 会选择“超限合并”以规避通信，从而优于 DINA。 |
| **增大模型层内存** | 使得单个合并操作极易跨越 EPC 界限 | 展现 DINA 在处理“极小超限”时的不灵活（被迫切断产生通信）。 |
| **拓扑复杂化** | 增加冗余路径 | Ours 算法在保持并行度方面的优势将进一步扩大。 |

## 总结

DINA 和 MEDIA 的一致性反映了**线性分区逻辑在处理非线性 DAG 时的局限性**。Ours 算法通过“保护并行结构”与“精细化任务划分”，在现代多核/多机分布式推理场景中具备更强的实用价值。
