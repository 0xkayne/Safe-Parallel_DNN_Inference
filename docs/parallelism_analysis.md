# 为什么 Ours 算法只在 InceptionV3 上有加速效果？—— 模型拓扑结构分析

## 1. 核心发现：模型并行度差异巨大

我对各模型的 DAG 图进行了拓扑分析，结果如下：

| 模型 | 节点数 | 边数 | **分叉点 (Fork)** | **汇聚点 (Join)** | 并行潜力 |
|------|--------|------|-------------------|-------------------|----------|
| **InceptionV3** | 181 | 211 | **11** | **11** | **高** |
| **BERT-base** | 123 | 122 | 1 | 0 | **极低** |
| 其他 Transformer | ~同上 | ~同上 | 0~1 | 0 | **无** |

*   **InceptionV3**: 拥有 **11 个分叉点**（Inception 模块的多分支结构）和 **11 个汇聚点**（Concat 层）。这些结构创造了真正的**任务级并行机会**。
*   **BERT-base**: 几乎是**纯线性链**。122 条边连接 123 个节点，意味着每个层只依赖前一个层。唯一的"分叉点"可能是输入层（这不构成有意义的并行）。

## 2. 问题定性：不是算法问题，是数据本身的特性

### 2.1 Transformer 模型的天然串行性
Transformer 架构（BERT、ViT、ALBERT 等）的核心计算流如下：
```
Input -> [Attention Block 1] -> [FFN 1] -> [Attention Block 2] -> ... -> Output
```
*   每个 Block 必须等待前一个 Block 完成。
*   **即使是 Self-Attention 内部的 Q/K/V 投影，在当前数据粒度下也被合并为单个"层"**，没有体现为独立的并行节点。

### 2.2 InceptionV3 的天然并行性
Inception 模块的结构：
```
Input -> [1x1 Conv] ----+
      -> [3x3 Conv] ----+-> Concat -> Output
      -> [5x5 Conv] ----+
      -> [MaxPool] -----+
```
*   四条分支在 `Input` 节点**分叉 (Fork)**。
*   四条分支在 `Concat` 节点**汇聚 (Join)**。
*   这四个分支**完全独立**，可以被 Ours 的调度器分配到 4 台不同的服务器上并行执行。

## 3. 结论：Ours 算法是正确的，只是测试模型不具备并行潜力

1.  **Ours 的设计意图**：识别并保护 DAG 中的并行结构，利用多服务器并行执行来加速。
2.  **当前结果的合理性**：
    *   在**无并行结构**的模型（BERT 等）上，没有加速空间，Ours 退守到 DINA 的基准性能（这是正确的行为）。
    *   在**有并行结构**的模型（InceptionV3）上，Ours 成功利用了 11 条并行路径，实现了 **10% 的加速**。
3.  **启示**：如果要进一步验证 Ours 的优势，应测试更多具有显式并行结构的模型，例如：
    *   **ResNeXt** (分组卷积)
    *   **EfficientNet** (多分支 MBConv)
    *   **NASNet** (复杂的多路径搜索结构)
    *   或者，**将 Transformer 的 Attention 内部拆分为 Q/K/V 三个独立节点**，这样也能创造并行机会。

## 4. 总结

三种算法在 Transformer 模型上表现相同，并不是 Ours 的"失败"，而是反映了一个物理事实：
> **无法从线性依赖链中凭空创造并行性。**

Ours 的价值在于：**当模型具有可利用的并行结构时（如 Inception），它能自动识别并加速；当模型没有时，它也不会比基准算法差。**
